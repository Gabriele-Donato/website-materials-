{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ed4a523-2ffa-406f-9b3c-0252b04df32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d56f9fb7-3e66-4227-a1a5-b8b593630a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter:\n",
    "    \"\"\"IMPORTANT: Note that while the predict step closely matches formula (3) in the paper, the update step uses the language and concepts\n",
    "    of the 'frequentist' derivation of the filter, e.g. kalman gain, innovation. Refer to Lacey's paper in Jones' article listed in the references \n",
    "    of my article. It is possible to show that the writing of the update step is equivalent to the formulas presented in the paper.\n",
    "    The use of the innovation and kalman_gain allows for a leaner code (a proof of equivalence will be included asap, it can be derived by equating\n",
    "    the two expressions).\"\"\"\n",
    "\n",
    "    def __init__(self, initial_state, state_covariance, transition_matrix, observation_matrix, process_noise, measurement_noise):\n",
    "        self.state = initial_state\n",
    "        self.state_covariance = state_covariance\n",
    "        self.transition_matrix = transition_matrix\n",
    "        self.observation_matrix = observation_matrix\n",
    "        self.process_noise = process_noise\n",
    "        self.measurement_noise = measurement_noise\n",
    "\n",
    "    def predict(self):\n",
    "        self.state = self.transition_matrix @ self.state\n",
    "        self.state_covariance = self.transition_matrix @ self.state_covariance @ self.transition_matrix.T + self.process_noise \n",
    "        \n",
    "    def update(self, observation):\n",
    "        predicted_observation = self.observation_matrix @ self.state\n",
    "        innovation = self.observation_matrix @ self.state_covariance @ self.observation_matrix.T + self.measurement_noise\n",
    "        kalman_gain = self.state_covariance @ self.observation_matrix.T @ np.linalg.inv(innovation)\n",
    "\n",
    "        #sampling from a normal distribution\n",
    "        posterior_mean = self.state + kalman_gain @ (observation - predicted_observation)\n",
    "        posterior_covariance = self.state_covariance - kalman_gain @ self.observation_matrix @ self.state_covariance\n",
    "        self.state = np.random.multivariate_normal(posterior_mean, posterior_covariance)\n",
    "        self.state_covariance = posterior_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bebef5d-f115-452f-ac1e-bb310305a304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced43de0-6e53-406e-bbc7-cb47d179e60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
