{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa6aa68-644f-4811-979a-0fa487dfec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"If you want to run the present code in the present version use Pycharm\"\"\"\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin, urlunparse\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe56413-3693-42ab-89d0-1100f31bf5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SitemapGenerator:\n",
    "    def __init__(self, root, filename, max_tasks=10, max_depth=3):\n",
    "        self.filename = filename\n",
    "        self.urls = {}\n",
    "        self.root = root\n",
    "        self.hostname = urlparse(root).hostname\n",
    "        self.semaphore = asyncio.Semaphore(max_tasks)\n",
    "        self.max_depth = max_depth\n",
    "        self.session = None\n",
    "\n",
    "    async def fetch(self, url):\n",
    "        async with self.semaphore:\n",
    "            try:\n",
    "                response = await self.session.get(url, timeout=10)\n",
    "                if response.status == 200:\n",
    "                    content = await response.text()\n",
    "                    return content\n",
    "                else:\n",
    "                    print(f\"Failed to fetch {url} with status {response.status}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def normalize_url(self, href, current_url):\n",
    "        if not href:\n",
    "            return None\n",
    "        parsed_href = urlparse(href)\n",
    "        if parsed_href.scheme and parsed_href.netloc:\n",
    "            return href if parsed_href.netloc == self.hostname else None\n",
    "        elif href.startswith(\"//\"):\n",
    "            return f\"https:{href}\" if self.hostname in href else None\n",
    "        return urljoin(current_url, href)\n",
    "\n",
    "    async def crawl(self, url, level):\n",
    "        if level > self.max_depth:\n",
    "            return\n",
    "        if url in self.urls:\n",
    "            return\n",
    "        print(f\"Level: {level} / Explore {url}\")\n",
    "        content = await self.fetch(url)\n",
    "        if content:\n",
    "            url = urlunparse(urlparse(url)._replace(fragment=''))\n",
    "            self.urls[url] = level\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            tasks = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = self.normalize_url(link.get('href'), url)\n",
    "                if href and href not in self.urls:\n",
    "                    tasks.append(self.crawl(href, level + 1))\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    async def run(self):\n",
    "        async with aiohttp.ClientSession() as self.session:\n",
    "            await self.crawl(self.root, 0)\n",
    "            self.generate_file()\n",
    "\n",
    "    def generate_file(self):\n",
    "        urlsbylevel = {}\n",
    "        for url, level in self.urls.items():\n",
    "            if level not in urlsbylevel:\n",
    "                urlsbylevel[level] = []\n",
    "            urlsbylevel[level].append(url)\n",
    "\n",
    "        maxlevel = max(urlsbylevel.keys(), default=0)\n",
    "        step = 1 / (maxlevel * 2 if maxlevel > 0 else 1)\n",
    "        root = ET.Element('urlset', xmlns='http://www.sitemaps.org/schemas/sitemap/0.9')\n",
    "\n",
    "        for level, urls in sorted(urlsbylevel.items()):\n",
    "            priority = round(1 - step * level, 2)\n",
    "            for url in urls:\n",
    "                url_element = ET.SubElement(root, \"url\")\n",
    "                ET.SubElement(url_element, \"loc\").text = url\n",
    "                ET.SubElement(url_element, \"priority\").text = str(priority)\n",
    "\n",
    "        tree = ET.ElementTree(root)\n",
    "        tree.write(self.filename, encoding=\"utf-8\", xml_declaration=True)\n",
    "        print(f\"Sitemap saved to '{self.filename}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049b2c8-ce9e-48e0-b381-c84d253a8c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_url = \"https://books.toscrape.com\"\n",
    "filename = \"sitemap.xml\"\n",
    "max_tasks = 20\n",
    "max_depth = 2\n",
    "generator = SitemapGenerator(root_url, filename, max_tasks, max_depth)\n",
    "asyncio.run(generator.run())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b82e527-b5c5-4b97-8cf7-33e0d5413627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
